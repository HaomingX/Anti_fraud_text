

[TOC]

# 语言模型的训练与预测

简单来说，语言模型就是通过统计已经分词的语料库中一元语法和二元语法出现的频次，实现对句子的分词。

下面我们使用HanLP的相关模块实现语言模型的训练和预测。

## 训练
训练是指根据给定样本集估计模型参数的过程，简单来说，在语言模型中就是统计二元语法出现的频次和一元语法出现的频次。

首先，我们使用HanLP的CorpusLoader.convert2SentenceList加载语料库（空格分词格式）。


```python
from pyhanlp import *
CorpusLoader = SafeJClass("com.hankcs.hanlp.corpus.document.CorpusLoader") 
corpus_path = r'D:\my_temp_doc\recently\dachaung\Introduction-NLP-master\Introduction-NLP-master\data\dictionnary\my_cws_corpus.txt'
sentences = CorpusLoader.convert2SentenceList(corpus_path) # 返回List<List<IWord>>类型
for sent in sentences:
    print(sent)
```

    [商品, 和, 服务]
    [商品, 和服, 物美价廉]
    [服务, 和, 货币]


接着，我们使用HanLP的NatureDictionaryMaker统计一元语法和二元语法


```python
NatureDictionaryMaker = SafeJClass("com.hankcs.hanlp.corpus.dictionary.NatureDictionaryMaker")  # 词典模型Java模块(统计一元、二元语法)

model_path = r"D:\my_temp_doc\recently\dachaung\Introduction-NLP-master\Introduction-NLP-master\data\dictionnary\my_cws_model"  # 语言模型存储路径
for sent in sentences:
    for word in sent:
        if word.label is None:
            word.setLabel("n")  # 赋予每个单词一个虚拟的n词性用作占位
maker = NatureDictionaryMaker()  # 构造NatureDictionaryMaker对象
maker.compute(sentences)  # 统计句子中的一元语法、二元语法
maker.saveTxtTo(model_path)  # 将统计结果存储到路径
```




    True



运行后在对应目录中，新建一元语法模型（my_cws_model.txt）、二元语法模型（my_cws_model.ngram.txt）和词性标注相关文件（my_cws_model.tr.txt）文件。

## 预测
预测是指利用模型对样本进行推断的过程，简单来说，就是通过我们之前统计的一元语法和二元语法的频次，推断句子的分词序列。
首先，我们使用HanLP的CoreDictionary和CoreBiGramTableDictionary加载刚才训练的语言模型


```python
HanLP.Config.CoreDictionaryPath = model_path + ".txt"  # 一元语法模型路径
HanLP.Config.BiGramDictionaryPath = model_path + ".ngram.txt"  # 二元语法模型路径
CoreDictionary = LazyLoadingJClass("com.hankcs.hanlp.dictionary.CoreDictionary")  # 加载一元语法模型Java模块
CoreBiGramTableDictionary = SafeJClass("com.hankcs.hanlp.dictionary.CoreBiGramTableDictionary")  # 加载二元语法模型Java模块
print(CoreDictionary.getTermFrequency("商品"))  # 测试"商品"的一元语法频次
print(CoreBiGramTableDictionary.getBiFrequency("商品", "和"))  # 测试"商品 和"的二元语法频次
```

    2
    1


词网是HanLP提出的特指句子中所有一元语法构成的网状结构的概念，其生成过程为：根据一元语法词典，将句子中所有单词找出来，并将起始位置相同的单词写作一行。例如，“商品和服务“的词网如下：


0:[ ]  
1:[商品]  
2:[]  
3:[和, 和服]  
4:[服务]  
5:[务]  
6:[ ]  

备注：markdown换行只需要在每行末尾多加两个空格

下面，我们使用HanLP的WordNet模块构建词网。


```python
from jpype import JString
WordNet = JClass("com.hankcs.hanlp.seg.common.WordNet")  # 构建词网的Java模块(词网模块)
Vertex = JClass("com.hankcs.hanlp.seg.common.Vertex")  # 构建词网的Java模块(词语存储对象)

sent = "货币和服务"
trie = CoreDictionary.trie

# 生成词网
searcher = trie.getSearcher(JString(sent), 0)
wordnet = WordNet(sent)
while searcher.next():
    wordnet.add(searcher.begin + 1,
                Vertex(sent[searcher.begin:searcher.begin + searcher.length], searcher.value, searcher.index))

# 补充一元语法中不包含但是保证图联通必须的词
vertexes = wordnet.getVertexes()
i = 0
while i < len(vertexes):
    if len(vertexes[i]) == 0:  # 空白行
        j = i + 1
        for j in range(i + 1, len(vertexes) - 1):  # 寻找第一个非空行 j
            if len(vertexes[j]):
                break
        wordnet.add(i, Vertex.newPunctuationInstance(sent[i - 1: j - 1]))  # 填充[i, j)之间的空白行
        i = j
    else:
        i += len(vertexes[i][-1].realWord)

print(wordnet)

# 没看懂
```

    0:[ ]
    1:[货币]
    2:[]
    3:[和, 和服]
    4:[服务]
    5:[务]
    6:[ ]


​    

最后，我们使用维特比算法来计算词网中的最短路径，以“货币和服务”的句子为例，得出其分词序列预测结果。


```python
def viterbi(wordnet):
    nodes = wordnet.getVertexes()
    # 前向遍历
    for i in range(0, len(nodes) - 1):
        for node in nodes[i]:
            for to in nodes[i + len(node.realWord)]:
                to.updateFrom(node)  # 根据距离公式计算节点距离，并维护最短路径上的前驱指针from
    # 后向回溯
    path = []  # 最短路径
    f = nodes[len(nodes) - 1].getFirst()  # 从终点回溯
    while f:
        path.insert(0, f)
        f = f.getFrom()  # 按前驱指针from回溯
    return [v.realWord for v in path]

print(viterbi(wordnet))
```

    [' ', '货币', '和服', '务', ' ']


上述词图每条边以二元语法的概率作为距离，那么中文分词任务转换为有向无环图上的最长路径问题。再通过将浮点数乘法转化为负对数之间的加法，相应的最长路径转化为负对数的最短路径。使用维特比算法求解。

## HanLP分词与用户词典的集成

词典往往廉价易得，资源丰富，利用统计模型的消歧能力，辅以用户词典处理新词，是提高分词器准确率的有效方式。HanLP支持 2 档用户词典优先级：

- **低优先级**：分词器首先在不考虑用户词典的情况下由统计模型预测分词结果，最后将该结果按照用户词典合并。默认低优先级。
- **高优先级**：分词器优先考虑用户词典，但具体实现由分词器子类自行决定。

**HanLP分词器简洁版**：


```python
from pyhanlp import *

ViterbiSegment = SafeJClass('com.hankcs.hanlp.seg.Viterbi.ViterbiSegment') #打开java模块

segment = ViterbiSegment()
sentence = "社会摇摆简称社会摇"
segment.enableCustomDictionary(False)
print("不挂载词典：", segment.seg(sentence))
CustomDictionary.insert("社会摇", "nz 100")
segment.enableCustomDictionary(True)
print("低优先级词典：", segment.seg(sentence))
segment.enableCustomDictionaryForcing(True)
print("高优先级词典：", segment.seg(sentence))
```

    不挂载词典： [社会摇摆简称社会摇/n]
    低优先级词典： [社会摇摆简称社会摇/n]
    高优先级词典： [社会摇/nz, 摆简称/n, 社会摇/nz]


可见，用户词典的高优先级未必是件好事，HanLP中的用户词典默认低优先级，做项目时请读者在理解上述说明的情况下根据实际需求自行开启高优先级。

## 二元语法与词典分词比较

按照NLP任务的一般流程，我们已经完成了语料标注和模型训练，现在来比较一下二元语法和词典分词的评测：

| 算法     | P     | R     | F1    | R(oov) | R(IV) |
| -------- | ----- | ----- | ----- | ------ | ----- |
| 最长匹配 | 89.41 | 94.64 | 91.95 | 2.58   | 97.14 |
| 二元语法 | 92.38 | 96.70 | 94.49 | 2.58   | 99.26 |

相较于词典分词，二元语法在精确度、召回率及IV召回率上全面胜出，最终F1值提高了 2.5%，成绩的提高主要受惠于消歧能力的提高。然而 OOV 召回依然是 n 元语法模型的硬伤，我们需要更强大的统计模型。

**关于精确度和召回率如果不清楚可以看下博客**

